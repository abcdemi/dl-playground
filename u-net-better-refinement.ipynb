{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573454b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# INSTALL DEPENDENCIES (RUNS ONCE AT THE START)\n",
    "# ==============================================================================\n",
    "print(\"Installing necessary libraries: torchmetrics, lpips, torch-fidelity...\")\n",
    "!pip install torchmetrics lpips torch-fidelity -q\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import torchvision.utils\n",
    "from torch_fidelity import calculate_metrics # Import the function directly\n",
    "\n",
    "# --- Setup and Device Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL ARCHITECTURE: U-NET WITH ASPP-BASED FEATURE REFINEMENT MODULE (FRM)\n",
    "# This is the upgraded, smarter version of the FRM.\n",
    "# ==============================================================================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x): return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(nn.MaxPool2d(2), DoubleConv(in_channels, out_channels))\n",
    "    def forward(self, x): return self.maxpool_conv(x)\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super(ASPPConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU()\n",
    "        )\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]; x = super(ASPPPooling, self).forward(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "class ASPPFeatureRefinementModule(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates=[3, 6, 9]):\n",
    "        super(ASPPFeatureRefinementModule, self).__init__()\n",
    "        out_channels = in_channels // 4\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU()\n",
    "        ))\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates: modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(len(self.convs) * out_channels, in_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, features, mask):\n",
    "        mask_downsampled = F.interpolate(mask, size=features.shape[2:], mode='nearest')\n",
    "        valid_features = features * (1 - mask_downsampled)\n",
    "        res = [conv(valid_features) for conv in self.convs]\n",
    "        res = torch.cat(res, dim=1)\n",
    "        refined_attention = self.project(res)\n",
    "        output = (features * (1 - mask_downsampled)) + (refined_attention * mask_downsampled)\n",
    "        return output\n",
    "\n",
    "class UpBlockWith_ASPP_FRM(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.frm = ASPPFeatureRefinementModule(in_channels) # Using the new FRM\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2, mask):\n",
    "        x1 = self.up(x1)\n",
    "        if x1.size() != x2.size():\n",
    "             x1 = F.interpolate(x1, size=x2.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x_refined = self.frm(x, mask)\n",
    "        return self.conv(x_refined)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__(); self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x): return self.conv(x)\n",
    "\n",
    "class UNetWith_ASPP_FRM(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=3):\n",
    "        super(UNetWith_ASPP_FRM, self).__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128); self.down2 = Down(128, 256); self.down3 = Down(256, 512)\n",
    "        self.up1 = UpBlockWith_ASPP_FRM(512, 256)\n",
    "        self.up2 = UpBlockWith_ASPP_FRM(256, 128)\n",
    "        self.up3 = UpBlockWith_ASPP_FRM(128, 64)\n",
    "        self.outc = OutConv(64, n_classes); self.final_activation = nn.Sigmoid()\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.inc(x); x2 = self.down1(x1); x3 = self.down2(x2); x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3, mask); x = self.up2(x, x2, mask); x = self.up3(x, x1, mask)\n",
    "        return self.final_activation(self.outc(x))\n",
    "\n",
    "# ==============================================================================\n",
    "# DATASET AND MASK GENERATION\n",
    "# (This section is unchanged)\n",
    "# ==============================================================================\n",
    "class InpaintingDataset(Dataset):\n",
    "    def __init__(self, cifar_dataset, mask_directory):\n",
    "        self.cifar_dataset = cifar_dataset; self.mask_dir = mask_directory\n",
    "    def __len__(self): return len(self.cifar_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.cifar_dataset[idx]\n",
    "        mask_path = os.path.join(self.mask_dir, f\"mask_{idx}.pt\")\n",
    "        mask = torch.load(mask_path)\n",
    "        return image, mask.unsqueeze(0)\n",
    "print(\"\\n--- STEP 1: Starting Mask Pre-computation ---\")\n",
    "MASK_DIR = './cifar10_masks/'\n",
    "os.makedirs(MASK_DIR, exist_ok=True)\n",
    "weights = DeepLabV3_ResNet50_Weights.DEFAULT\n",
    "segmentation_model = deeplabv3_resnet50(weights=weights).to(device).eval()\n",
    "preprocess_seg = weights.transforms()\n",
    "cifar10_for_masks = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mask_gen_loader = DataLoader(cifar10_for_masks, batch_size=16, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    image_index = 0\n",
    "    for images, _ in tqdm(mask_gen_loader, desc=\"Generating and Saving Masks\"):\n",
    "        images = images.to(device)\n",
    "        output = segmentation_model(preprocess_seg(images))['out']\n",
    "        seg_maps = torch.argmax(output, dim=1)\n",
    "        for seg_map in seg_maps:\n",
    "            foreground_ids = torch.unique(seg_map)[torch.unique(seg_map) > 0]\n",
    "            mask = torch.zeros_like(seg_map, dtype=torch.float32)\n",
    "            if len(foreground_ids) > 0:\n",
    "                mask[seg_map == np.random.choice(foreground_ids.cpu().numpy())] = 1.0\n",
    "            mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=(32, 32), mode='nearest').squeeze().cpu()\n",
    "            torch.save(mask, f\"{MASK_DIR}/mask_{image_index}.pt\")\n",
    "            image_index += 1\n",
    "print(f\"--- Finished. {image_index} masks saved. ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: TRAIN THE U-NET WITH ASPP-FRM MODEL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- STEP 2: Starting U-Net with ASPP-FRM Training ---\")\n",
    "train_data_original = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_dataset = InpaintingDataset(cifar_dataset=train_data_original, mask_directory=MASK_DIR)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
    "inpainting_model = UNetWith_ASPP_FRM(n_channels=3, n_classes=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(inpainting_model.parameters(), lr=1e-3)\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "    for original_imgs, masks in progress_bar:\n",
    "        original_imgs, masks = original_imgs.to(device), masks.to(device)\n",
    "        masked_imgs = original_imgs * (1 - masks)\n",
    "        outputs = inpainting_model(masked_imgs, masks)\n",
    "        loss = criterion(outputs, original_imgs)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': running_loss / (progress_bar.n + 1)})\n",
    "print(\"--- Finished Training ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: VISUALIZE RESULTS\n",
    "# (This section is unchanged)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- STEP 3: Visualizing Results on a Test Batch ---\")\n",
    "inpainting_model.eval()\n",
    "test_data_original = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader_sample = DataLoader(test_data_original, batch_size=10, shuffle=True)\n",
    "original_test_imgs, _ = next(iter(test_loader_sample))\n",
    "original_test_imgs = original_test_imgs.to(device)\n",
    "with torch.no_grad():\n",
    "    output = segmentation_model(preprocess_seg(original_test_imgs))['out']\n",
    "    seg_maps = torch.argmax(output, dim=1)\n",
    "    test_masks = []\n",
    "    for seg_map in seg_maps:\n",
    "        foreground_ids = torch.unique(seg_map)[torch.unique(seg_map) > 0]\n",
    "        mask = torch.zeros_like(seg_map, dtype=torch.float32)\n",
    "        if len(foreground_ids) > 0:\n",
    "            mask[seg_map == np.random.choice(foreground_ids.cpu().numpy())] = 1.0\n",
    "        test_masks.append(F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=(32, 32), mode='nearest'))\n",
    "    test_masks = torch.cat(test_masks, dim=0).to(device)\n",
    "    masked_test_imgs = original_test_imgs * (1 - test_masks)\n",
    "    inpainted_imgs = inpainting_model(masked_test_imgs, test_masks)\n",
    "n = 10; plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    def plot_image(position, img_tensor, title):\n",
    "        ax = plt.subplot(4, n, position)\n",
    "        plt.imshow(img_tensor.cpu().permute(1, 2, 0))\n",
    "        ax.get_xaxis().set_visible(False); ax.get_yaxis().set_visible(False)\n",
    "        if i == 0: ax.set_title(title, fontsize=14)\n",
    "    plot_image(i + 1, original_test_imgs[i], \"Original\"); plot_image(i + 1 + n, test_masks[i], \"Mask\")\n",
    "    plot_image(i + 1 + 2 * n, masked_test_imgs[i], \"Masked Input\"); plot_image(i + 1 + 3 * n, inpainted_imgs[i], \"Inpainted Result\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: CALCULATE ALL METRICS (PSNR, SSIM, LPIPS, FID)\n",
    "# (This section is unchanged and uses the final, robust FID calculation)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- STEP 4: Starting Quantitative Evaluation on Full Test Set ---\")\n",
    "import torchmetrics; import lpips\n",
    "psnr_metric = torchmetrics.PeakSignalNoiseRatio().to(device)\n",
    "ssim_metric = torchmetrics.StructuralSimilarityIndexMeasure().to(device)\n",
    "lpips_metric = torchmetrics.image.lpip.LearnedPerceptualImagePatchSimilarity(net_type='vgg').to(device)\n",
    "REAL_PATCHES_DIR = '/tmp/real_patches/'; FAKE_PATCHES_DIR = '/tmp/fake_patches/'\n",
    "os.makedirs(REAL_PATCHES_DIR, exist_ok=True); os.makedirs(FAKE_PATCHES_DIR, exist_ok=True)\n",
    "shutil.rmtree(REAL_PATCHES_DIR); shutil.rmtree(FAKE_PATCHES_DIR)\n",
    "os.makedirs(REAL_PATCHES_DIR); os.makedirs(FAKE_PATCHES_DIR)\n",
    "test_loader_full = DataLoader(test_data_original, batch_size=32, shuffle=False)\n",
    "patch_count = 0\n",
    "with torch.no_grad():\n",
    "    for original_imgs, _ in tqdm(test_loader_full, desc=\"Evaluating Metrics and Saving Patches\"):\n",
    "        original_imgs = original_imgs.to(device)\n",
    "        output = segmentation_model(preprocess_seg(original_imgs))['out']\n",
    "        seg_maps = torch.argmax(output, dim=1)\n",
    "        masks = []\n",
    "        for seg_map in seg_maps:\n",
    "            foreground_ids = torch.unique(seg_map)[torch.unique(seg_map) > 0]\n",
    "            mask = torch.zeros_like(seg_map, dtype=torch.float32)\n",
    "            if len(foreground_ids) > 0:\n",
    "                mask[seg_map == np.random.choice(foreground_ids.cpu().numpy())] = 1.0\n",
    "            masks.append(F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=(32, 32), mode='nearest'))\n",
    "        masks = torch.cat(masks, dim=0).to(device)\n",
    "        masked_imgs = original_imgs * (1 - masks)\n",
    "        inpainted_imgs = inpainting_model(masked_imgs, masks)\n",
    "        psnr_metric.update(inpainted_imgs, original_imgs)\n",
    "        ssim_metric.update(inpainted_imgs, original_imgs)\n",
    "        lpips_metric.update(inpainted_imgs * 2 - 1, original_imgs * 2 - 1)\n",
    "        for i in range(original_imgs.size(0)):\n",
    "            single_mask = masks[i].squeeze()\n",
    "            if not torch.any(single_mask): continue\n",
    "            rows, cols = torch.any(single_mask, axis=1), torch.any(single_mask, axis=0)\n",
    "            rmin, rmax = torch.where(rows)[0][[0, -1]]; cmin, cmax = torch.where(cols)[0][[0, -1]]\n",
    "            real_patch = original_imgs[i:i+1, :, rmin:rmax+1, cmin:cmax+1]\n",
    "            fake_patch = inpainted_imgs[i:i+1, :, rmin:rmax+1, cmin:cmax+1]\n",
    "            real_patch_resized = F.interpolate(real_patch, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "            fake_patch_resized = F.interpolate(fake_patch, size=(32, 32), mode='bilinear', align_corners=False)\n",
    "            torchvision.utils.save_image(real_patch_resized, os.path.join(REAL_PATCHES_DIR, f'p_{patch_count}.png'))\n",
    "            torchvision.utils.save_image(fake_patch_resized, os.path.join(FAKE_PATCHES_DIR, f'p_{patch_count}.png'))\n",
    "            patch_count += 1\n",
    "final_psnr = psnr_metric.compute(); final_ssim = ssim_metric.compute(); final_lpips = lpips_metric.compute()\n",
    "fid_score = \"N/A\"\n",
    "if patch_count > 1:\n",
    "    print(f\"\\nSaved {patch_count} uniform patches. Calculating FID score...\")\n",
    "    try:\n",
    "        metrics_dict = calculate_metrics(\n",
    "            input1=REAL_PATCHES_DIR, input2=FAKE_PATCHES_DIR, cuda=torch.cuda.is_available(), fid=True, verbose=False,\n",
    "        )\n",
    "        fid_score = metrics_dict['frechet_inception_distance']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during FID calculation: {e}\")\n",
    "else:\n",
    "    print(\"Not enough patches generated to calculate FID.\")\n",
    "print(\"\\n\\n--- FINAL EVALUATION REPORT (U-Net with ASPP-FRM) ---\")\n",
    "print(f\"Peak Signal-to-Noise Ratio (PSNR): {final_psnr:.4f} dB (Higher is better)\")\n",
    "print(f\"Structural Similarity Index (SSIM): {final_ssim:.4f} (Higher is better)\")\n",
    "print(f\"Learned Perceptual Patch Similarity (LPIPS): {final_lpips:.4f} (Lower is better)\")\n",
    "print(f\"Fréchet Inception Distance (FID on patches): {fid_score if isinstance(fid_score, str) else f'{fid_score:.4f}'} (Lower is better)\")\n",
    "if os.path.exists(REAL_PATCHES_DIR): shutil.rmtree(REAL_PATCHES_DIR)\n",
    "if os.path.exists(FAKE_PATCHES_DIR): shutil.rmtree(FAKE_PATCHES_DIR)\n",
    "if os.path.exists(MASK_DIR): shutil.rmtree(MASK_DIR)\n",
    "print(\"\\nCleaned up temporary directories.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
